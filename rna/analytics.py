"""
Performs project specific.
"""
import os

import numpy as np
from sklearn.metrics import accuracy_score

from lir.plotting import makeplot_hist_density
from rna.constants import single_cell_types

from lir.lr import calculate_cllr
from rna.nfold_analysis import model_with_correct_settings
from rna.plotting import plot_histogram_log_lr


def combine_samples(data_for_class):
    """
    Combines the repeated measurements for each sample.

    :param data_for_class: N_samples x N_observations_per_sample x N_markers measurements numpy array
    :return: N_samples x N_markers measurements numpy array
    """
    data_for_class_mean = np.array([np.mean(data_for_class[i], axis=0)
                                    for i in range(data_for_class.shape[0])])
    return data_for_class_mean


def construct_random_samples(X, y, n, classes_to_include, n_features):
    """
    Returns n generated samples that contain classes classes_to_include.
    A sample is generated by random sampling a sample for each class, and adding
    the shuffled replicates.

    :param X: N_single_cell_experimental_samples array and within a list filled with
        for each n_single_cell_experimental_sample a N_measurements per sample x N_markers array
    :param y: list of length N_single_cell_experimental_samples filled with int labels of which
        cell type was measured
    :param n: number of samples to generate
    :param classes_to_include: iterable of int, cell type indices to include
        in the mixtures
    :param n_features: int N_markers (=N_features)
    :return: n x n_features array
    """
    if len(classes_to_include) == 0:
        return np.zeros((n, n_features))
    data_for_class=[]
    for clas in classes_to_include:
        data_for_class.append(X[np.argwhere(np.array(y) == clas)[:, 0]])

    augmented_samples = []
    for i in range(n):
        sampled = []
        for j, clas in enumerate(classes_to_include):

            n_in_class = sum(np.array(y) == clas)
            sampled_sample = data_for_class[j][np.random.randint(n_in_class)]
            n_replicates = len(sampled_sample)
            sampled.append(sampled_sample[np.random.permutation(n_replicates)])
        # TODO thus lower replicates for more cell types. is this an issue?
        smallest_replicates = min([len(sample) for sample in sampled])

        combined_sample = []
        for i_replicate in range(smallest_replicates):
            # TODO: For now chose to take the sum. Perhaps another way to combine the samples?
            # combined_sample.append(np.sum(np.array([sample[i_replicate] for sample in sampled]), axis=0))
            combined_sample.append(np.mean(np.array([sample[i_replicate] for sample in sampled]), axis=0))

        augmented_samples.append(combined_sample)
    return combine_samples(np.array(augmented_samples))


def augment_data(X, y, n_celltypes, n_features, N_SAMPLES_PER_COMBINATION,
                 label_encoder, binarize=False, from_penile=False):
    """
    Generate data for the power set of single cell types

    :param X: n_samples x n_measurements per sample x n_markers array of measurements
    :param y_nhot: n_samples x n_celltypes_with_penile array of int labels of which
        cell type was measured
    :param n_celltypes: int: number of single cell types
    :param n_features: int: n_markers
    :param N_SAMPLES_PER_COMBINATION:
    :param label_encoder:
    :param from_penile: bool: generate sample that (T) always or (F) never
        also contain penile skin
    :return: n_experiments x n_markers array,
             n_experiments x n_celltypes matrix of 0, 1 indicating for each augmented sample
                which single cell type it was made up of. Does not contain column for penile skin
    """

    if X.size == 0:
        # This is the case when calibration_size = 0.0, this is an implicit way to
        # ensure that calibration is not performed.
        X_augmented=None
        y_nhot_augmented=np.zeros((0, n_celltypes))

    else:
        X_augmented = np.zeros((0, n_features))
        y_nhot_augmented = np.zeros((2 ** n_celltypes * N_SAMPLES_PER_COMBINATION,
                                     n_celltypes), dtype=int)

        # for each possible combination augment N_SAMPLES_PER_COMBINATION
        for i in range(2 ** n_celltypes):
            binary = bin(i)[2:]
            while len(binary) < n_celltypes:
                binary = '0' + binary

            classes_in_current_mixture = []
            for i_celltype in range(len(label_encoder.classes_)):
                if binary[-i_celltype - 1] == '1':
                    classes_in_current_mixture.append(i_celltype)
                    y_nhot_augmented[i * N_SAMPLES_PER_COMBINATION:(i + 1) * N_SAMPLES_PER_COMBINATION, i_celltype] = 1
            if from_penile:
                # also (always) add penile skin samples. the index for penile is n_celltypes
                y_nhot_augmented[i * N_SAMPLES_PER_COMBINATION:(i + 1) * N_SAMPLES_PER_COMBINATION, n_celltypes] = 1
                classes_in_current_mixture.append(n_celltypes)

            X_augmented = np.append(X_augmented, construct_random_samples(
                X, y, N_SAMPLES_PER_COMBINATION, classes_in_current_mixture, n_features), axis=0)

        if binarize:
            X_augmented = np.where(X_augmented > 150, 1, 0)
        else: # normalize
            X_augmented = X_augmented / 1000
            # X_augmented = normalize(X_augmented)

    return X_augmented, y_nhot_augmented[:, :n_celltypes]


def get_mixture_columns_for_class(target_class, priors):
    """
    for the target_class, a vector of length n_single_cell_types with 1 or more 1's, give
    back the columns in the mixtures that contain one or more of these single cell types

    :param target_class: vector of length n_single_cell_types with at least one 1
    :param priors: vector of length n_single_cell_types with 0 or 1 to indicate single cell type has 0 or 1 prior,
    uniform assumed otherwise
    :return: list of ints, in [0, 2 ** n_cell_types]
    """

    def int_to_binary(i):
        binary = bin(i)[2:]
        while len(binary) < len(single_cell_types):
            binary = '0' + binary
        return np.flip([int(j) for j in binary]).tolist()

    def binary_admissable(binary, target_class, priors):
        """
        gives back whether the binary (string of 0 and 1 of length n_single_cell_types) has at least one of
        target_class in it, and all priors satisfied
        """
        if priors:
            for i in range(len(target_class)):
                # if prior is zero, the class should not occur
                if binary[i] == 1 and priors[i] == 0:
                    return False
                # if prior is one, the class should occur
                # as binary is zero it does not occur and return False
                if binary[i] == 0 and priors[i] == 1:
                    return False
        # at least one of the target class should occur
        if np.inner(binary, target_class)==0:
            return False
        return True

    return [i for i in range(2 ** len(single_cell_types)) if binary_admissable(int_to_binary(i), target_class, priors)]


def cllr(lrs, y_nhot, target_class):
    """
    Computes the cllr for one celltype.

    :param lrs: numpy array: N_samples with the LRs from the method
    :param y_nhot: N_samples x N_single_cell_type n_hot encoding of the labels
    :param target_class: vector of length n_single_cell_types with at least one 1
    :return: float: the log-likehood cost ratio
    """

    lrs1 = np.multiply(lrs, np.max(np.multiply(y_nhot, target_class), axis=1))
    lrs2 = np.multiply(lrs, 1 - np.max(np.multiply(y_nhot, target_class), axis=1))

    # delete zeros
    lrs1 = np.delete(lrs1, np.where(lrs1 == 0.0))
    lrs2 = np.delete(lrs2, np.where(lrs2 == 0.0))

    if len(lrs1) > 0 and len(lrs2) > 0:
        return calculate_cllr(lrs2, lrs1).cllr
    else:
        return 9999.0000


def perform_analysis(n, binarize, softmax, models, mle, label_encoder, X_train_augmented, y_train_nhot_augmented,
                     X_calib_augmented, y_calib_nhot_augmented, X_test_augmented, y_test_nhot_augmented,
                     X_test_as_mixtures_augmented, X_mixtures, target_classes, save_hist):

    classifier = models[0]
    with_calibration = models[1]

    model = model_with_correct_settings(classifier, softmax)

    if with_calibration: # with calibration
        lrs_before_calib, lrs_after_calib, lrs_test_as_mixtures_before_calib, lrs_test_as_mixtures_after_calib, lrs_before_calib_mixt, lrs_after_calib_mixt = \
            generate_lrs(model, mle, softmax, X_train_augmented, y_train_nhot_augmented, X_calib_augmented,
                         y_calib_nhot_augmented, X_test_augmented, X_test_as_mixtures_augmented, X_mixtures, target_classes)

        if save_hist:
            plot_histogram_log_lr(lrs_before_calib, y_test_nhot_augmented, target_classes, label_encoder, density=True,
                                  savefig=os.path.join('scratch', 'hist_before_{}_{}_{}_{}'.format(n, binarize, softmax, classifier)))
            plot_histogram_log_lr(lrs_after_calib, y_test_nhot_augmented, target_classes, label_encoder, density=True,
                                  title='after', savefig=os.path.join('scratch', 'hist_after_{}_{}_{}_{}'.format(n, binarize, softmax, classifier)))
            makeplot_hist_density(model.predict_lrs(X_calib_augmented, target_classes, with_calibration=False),
                              y_calib_nhot_augmented, model._calibrators_per_target_class, target_classes,
                              label_encoder, savefig=os.path.join('scratch', 'kernel_density_estimation{}_{}_{}_{}'.format(n, binarize, softmax, classifier)))

    else: # no calibration
        lrs_before_calib, lrs_after_calib, lrs_test_as_mixtures_before_calib, lrs_test_as_mixtures_after_calib, lrs_before_calib_mixt, lrs_after_calib_mixt = \
            generate_lrs(model, mle, softmax, np.concatenate((X_train_augmented, X_calib_augmented), axis=0),
                         np.concatenate((y_train_nhot_augmented, y_calib_nhot_augmented), axis=0), np.array([]),
                         np.array([]), X_test_augmented, X_test_as_mixtures_augmented, X_mixtures, target_classes)

        if save_hist:
            plot_histogram_log_lr(lrs_before_calib, y_test_nhot_augmented, target_classes, label_encoder, density=True,
                                  savefig=os.path.join('scratch', 'hist_before_{}_{}_{}_{}'.format(n, binarize, softmax, classifier)))

    return model, lrs_before_calib, lrs_after_calib, lrs_test_as_mixtures_before_calib, \
           lrs_test_as_mixtures_after_calib, lrs_before_calib_mixt, lrs_after_calib_mixt


def generate_lrs(model, mle, softmax, X_train, y_train, X_calib, y_calib, X_test, X_test_as_mixtures, X_mixtures, target_classes):
    """
    When softmax the model must be fitted on labels, whereas with sigmoid the model must be fitted on
    an nhot encoded vector representing the labels. Ensure that labels take the correct form, fit the
    model and predict the lrs before and after calibration for both X_test and X_mixtures.
    """

    if softmax: # y_train must be list with labels
        try:
            y_train = mle.nhot_to_labels(y_train)
        except: # already are labels
            pass
    else: # y_train must be nhot encoded labels
        try:
            y_train = mle.labels_to_nhot(y_train)
        except: # already is nhot encoded
            pass
        indices = [np.argwhere(target_classes[i, :] == 1).flatten().tolist() for i in range(target_classes.shape[0])]
        y_train = np.array([np.max(np.array(y_train[:, indices[i]]), axis=1) for i in range(len(indices))]).T

    try: # y_calib must always be nhot encoded
        y_calib = mle.labels_to_nhot(y_calib)
    except: # already is nhot encoded
        pass

    model.fit_classifier(X_train, y_train)
    model.fit_calibration(X_calib, y_calib, target_classes)

    lrs_before_calib = model.predict_lrs(X_test, target_classes, with_calibration=False)
    lrs_after_calib = model.predict_lrs(X_test, target_classes)

    lrs_reduced_before_calib = model.predict_lrs(X_test_as_mixtures, target_classes, with_calibration=False)
    lrs_reduced_after_calib = model.predict_lrs(X_test_as_mixtures, target_classes)

    lrs_before_calib_mixt = model.predict_lrs(X_mixtures, target_classes, with_calibration=False)
    lrs_after_calib_mixt = model.predict_lrs(X_mixtures, target_classes)

    return lrs_before_calib, lrs_after_calib, lrs_reduced_before_calib, lrs_reduced_after_calib, \
           lrs_before_calib_mixt, lrs_after_calib_mixt


def get_accuracy(model, mle, y_true, X, target_classes):
    """
    Predicts labels and ensures that both the true and predicted labels are nhot encoded.
    Calculates the accuracy.

    :return: accuracy: the set of labels predicted for a sample must *exactly* match the
        corresponding set of labels in y_true.
    """

    y_pred = model._classifier.predict(X)

    try:
        y_true = mle.labels_to_nhot(y_true)
    except:
        pass

    try:
        y_pred = mle.labels_to_nhot(y_pred)
    except:
        pass

    if y_true.shape[1] != target_classes.shape[0] and y_pred.shape[1] == target_classes.shape[0]:
        indices = [np.argwhere(target_classes[i, :] == 1).flatten().tolist() for i in range(target_classes.shape[0])]
        y_true = np.array([np.max(np.array(y_true[:, indices[i]]), axis=1) for i in range(len(indices))]).T

    return accuracy_score(y_true, y_pred)